#include "vm.h"
#include "mm.h"
#include "device.h"
#include "symbol.h"
#include "string.h"
#include "stdio.h"

extern pagetable_t kpgtbl;  // 

void kvminit() {

  char * tempsscratch = kalloc();
 // write_csr(sscratch,PA2VA(tempsscratch + PAGE_SIZE));
  write_csr(sscratch,(tempsscratch + PAGE_SIZE));
  printf("tempsscratch kalloc() done!\n");

  kpgtbl = (pagetable_t) kalloc();
  printf("kvminit(): kpgtbl = %4x\n",kpgtbl);
  memset(kpgtbl, 0, PAGE_SIZE);
    
  // map devices
  uint64 uart = PA2VA(get_device_addr(UART_MMIO));
  kvmmap(kpgtbl, uart, VA2PA(uart), get_device_size(UART_MMIO), (PTE_R | PTE_W ));

  uint64 poweroff = PA2VA(get_device_addr(POWEROFF_MMIO));
  kvmmap(kpgtbl, poweroff, VA2PA(poweroff), get_device_size(POWEROFF_MMIO), (PTE_R | PTE_W ));

 // kvmmap(kpgtbl, (uint64)&bss_start, VA2PA((uint64)&bss_start), (uint64)&bss_end - (uint64)&bss_start, (PTE_R | PTE_X | PTE_W));

  // map kernel text executable and read-only.
  kvmmap(kpgtbl, (uint64)&text_start, VA2PA((uint64)&text_start), (uint64)&text_end - (uint64)&text_start, (PTE_R | PTE_X ));
  // map kernel data and the physical RAM we'll make use of.
  kvmmap(kpgtbl, (uint64)&rodata_start, VA2PA((uint64)&rodata_start), (uint64)&rodata_end - (uint64)&rodata_start, (PTE_R | PTE_W));
  kvmmap(kpgtbl, (uint64)&data_start, VA2PA((uint64)&data_start), (uint64)PHY_END - VA2PA((uint64)&data_start), (PTE_R | PTE_W | PTE_X));

  
//  kvmmap(kpgtbl, 0x00000000, 0x84000000, PAGE_SIZE, (PTE_R | PTE_W | PTE_X));

  
 // map kernel stacks

 // disable the MMU reopen after at end of task_init
	
 // unsigned long long zero = 0x00000000;

 // write_csr(satp, zero);

  write_csr(satp, MAKE_SATP(kpgtbl));

  asm volatile("sfence.vma");
}



// Return the address of the PTE in page table pagetable
// that corresponds to virtual address va.  If alloc!=0,
// create any required page-table pages.
pte_t * walk(pagetable_t pagetable, uint64 va, int alloc)
{

  for(int level = 2; level > 0; level--) {
    pte_t *pte = &pagetable[PX(level, va)];
    if(*pte & PTE_V) {
      pagetable = (pagetable_t)PTE2PA(*pte);
    } else {
      if(!alloc || (pagetable = (pagetable_t)kalloc()) == 0) // kalloc
        return 0;
      memset(pagetable, 0, PAGE_SIZE);
      *pte = PA2PTE(pagetable) | PTE_V;
    }
  }
  return &pagetable[PX(0, va)];
}

pte_t * vwalk(pagetable_t pagetable, uint64 va, int alloc){
	for(int level = 2; level > 0; level--) {
    pte_t *pte = &pagetable[PX(level, va)];
    if(*pte & PTE_V) {
      pagetable = (pagetable_t)PA2VA(PTE2PA(*pte));
    } else {
      if(!alloc || (pagetable = (pagetable_t)alloc_page()) == 0) // alloc
        return 0;
      memset(pagetable, 0, PAGE_SIZE);
      *pte = PA2PTE(VA2PA(pagetable)) | PTE_V;
    }
  }
  return &pagetable[PX(0, va)];
}

// Create PTEs for virtual addresses starting at va that refer to
// physical addresses starting at pa. va and size might not
// be page-aligned. Returns 0 on success, -1 if walk() couldn't
// allocate a needed page-table page.
int mappages(pagetable_t pagetable, uint64 va, uint64 size, uint64 pa, int perm)
{
  uint64 a, last;
  pte_t *pte;
//  printf("mappages():map begin...\n");
  
  a = PAGE_DOWN(va);
  last = PAGE_DOWN(va + size - 1);
  
  for(;;){
//  printf("mappages(): begin = %4x\n",a);
//  printf("mappages(): last = %4x\n",last);
    if((pte = walk(pagetable, a, PAGE_ALLOC)) == 0){
	panic("walk() cannot allocate needed page-table page");
	return -1;	
    }
//	printf("mappages():walk-PASS\n");
    if(*pte & PTE_V)
      panic("remap");
    *pte = PA2PTE(pa) | perm | PTE_V;
    if(a == last)
      break;
    a += PAGE_SIZE;
    pa += PAGE_SIZE;
  }
//  printf("mappages():map done!\n");
  return 0;
}

//same as mappages , below just work in virtual address environment
int vmappages(pagetable_t pagetable, uint64 va, uint64 size, uint64 pa, int perm)
{
  uint64 a, last;
  pte_t *pte;
 // printf("vmappages():map begin...\n");
  
  a = PAGE_DOWN(va);
  last = PAGE_DOWN(va + size - 1);
  
  for(;;){
//  printf("mappages(): begin = %4x\n",a);
//  printf("mappages(): last = %4x\n",last);
    if((pte = vwalk(pagetable, a, PAGE_ALLOC)) == 0){
	panic("vwalk() cannot allocate needed page-table page");
	return -1;	
    }
//	printf("vmappages():walk-PASS\n");
    if(*pte & PTE_V)
      panic("remap");
    *pte = PA2PTE(pa) | perm | PTE_V;
    if(a == last)
      break;
    a += PAGE_SIZE;
    pa += PAGE_SIZE;
  }
 // printf("vmappages():map done!\n");
  return 0;
}

// add a mapping to the kernel page table.
void kvmmap(pagetable_t kpgtbl, uint64 va, uint64 pa, uint64 sz, int perm)
{
  if(mappages(kpgtbl, va, sz, pa, perm) != 0)
    panic("kvmmap");
}

void vkvmmap(pagetable_t kpgtbl, uint64 va, uint64 pa, uint64 sz, int perm)
{
  if(vmappages(kpgtbl, va, sz, pa, perm) != 0)
    panic("kvmmap");
}

void uvminit(pagetable_t pagetable, uchar* src, uint sz)
{
    char* mem;

    if (sz > PAGE_SIZE)
        panic("inituvm: more than a page");
    mem = alloc_page();
    memset(mem, 0, PAGE_SIZE);
    vmappages(pagetable, 0, PAGE_SIZE, (uint64)mem, PTE_W | PTE_R | PTE_X | PTE_U);
	    
	//memmove(mem, src, sz);
	memcpy(mem , src , sz);
	printf("memmove ok\n");
	return;
}

pagetable_t uvmcreate(void)
{
  pagetable_t pagetable;
  pagetable = (pagetable_t) kalloc();
//  pagetable = (pagetable_t)alloc_page();
  if(pagetable == 0)
    return 0;
  memset(pagetable, 0, PAGE_SIZE);
  return pagetable;
}
